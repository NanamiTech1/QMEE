<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Bayesian approaches</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="qmee.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header class="site-header">
<div class="wrapper">
	<h1 class="title">
		<a href="index.html">Biology 708, McMaster University</a>
	</h1>
	<div style="text-align:center">
		<img src="http://imgs.xkcd.com/comics/self_description.png">
	</div>
</div>
</header>
<header id="title-block-header">
<h1 class="title">Bayesian approaches</h1>
</header>
<h1 id="introduction">Introduction</h1>
<h2 id="bayes-theorem">Bayes Theorem</h2>
<ul>
<li><p>Bayes theorem is what we used for the MMV calculation</p></li>
<li>If <span class="math inline">\(A_i\)</span> are <em>alternative events</em> (exactly one must happen), then:
<ul>
<li><span class="math inline">\(\mbox{Pr}(A_i|B) = \frac{\mbox{Pr}(B|A_i) \mbox{Pr}(A_i)}{\sum{\mbox{Pr}(B|A_j) \mbox{Pr}(A_j)}}\)</span></li>
<li><span class="math inline">\(\mbox{Pr}(A_i)\)</span> the <em>prior</em> probability of <span class="math inline">\(A_i\)</span></li>
<li><span class="math inline">\(\mbox{Pr}(A_i|B)\)</span> is the <em>posterior</em> probability of <span class="math inline">\(A_i\)</span>, given event <span class="math inline">\(B\)</span></li>
</ul></li>
<li><p>People argue about Bayesian inference, but nobody argues about Bayes theorem</p></li>
</ul>
<h2 id="bayesian-inference">Bayesian inference</h2>
<ul>
<li>Go from a <em>statistical model</em> of how your data are generated, to a probability model of parameter values
<ul>
<li>Requires <em>prior</em> distributions describing the assumed likelihood of parameters before these observations are made</li>
<li>Use Bayes theorem to go from probability of the data given parameters to the probability of parameters given data</li>
</ul></li>
<li>Once we have a posterior distribution, we can calculate a best guess for each parameter
<ul>
<li>Mean, median or mode</li>
<li>Only median is scale-independent</li>
</ul></li>
</ul>
<h2 id="confidence-intervals">Confidence intervals</h2>
<ul>
<li>We do hypothesis tests using “credible intervals” – these are like confidence intervals, except that we really believe (relying on our assumptions) that there is a 95% chance that the value is in the credible interval
<ul>
<li>There are a lot of ways to do this. You need to decide in advance.</li>
<li><em>Quantiles</em> are principled, but not easy in &gt;1 dimension</li>
<li>Highest posterior density is straightforward, but scale-dependent</li>
</ul></li>
<li><p>Example, a linear relationship is significant if the credible interval for the slope does not include zero</p></li>
<li><p>A difference between groups is significant if the credible interval for the difference does not include zero</p></li>
</ul>
<h2 id="advantages">Advantages</h2>
<ul>
<li><p>Assumptions more explicit</p></li>
<li><p>Probability statements more straightforward</p></li>
<li><p>Very flexible</p></li>
<li><p>Can combine information from different sources</p></li>
</ul>
<h2 id="disadvantages">Disadvantages</h2>
<ul>
<li><p>More assumptions required</p></li>
<li>More difficult to calculate answers
<ul>
<li>easy problems are easy</li>
<li>medium problems are hard [compared to the frequentist analog]</li>
<li>hard problems are possible [not always true for frequentist analog]</li>
</ul></li>
</ul>
<h1 id="assumptions">Assumptions</h1>
<h2 id="prior-distributions">Prior distributions</h2>
<ul>
<li>Typically, start with a prior distribution that has little “information”
<ul>
<li>Let the data do the work</li>
</ul></li>
<li>This often means a normal (or lognormal, or gamma) with a very large variance
<ul>
<li>We can test for sensitivity to this choice</li>
</ul></li>
<li>Can also use a uniform distribution (on log, or linear scale) with very broad coverage</li>
</ul>
<h2 id="examples">Examples</h2>
<ul>
<li><p>“Complete ignorance” can be harder to specify than you think</p>
<ul>
<li><p>Linear vs. log scale: do we expect the probability of being between 10 and 11 grams to be the same as the prob. of being between 100 and 101 grams, or the same as the prob. of being between 100 and 110 grams??</p></li>
<li><p>Linear vs. inverse scale: if we are waiting for things to happen, do we pick our prior on the time scale (number of minutes per bus) or the rate scale (number of buses per minute)?</p></li>
<li><p>Discrete hypotheses: subdivision (nest predation example: do we consider species separately, or grouped by higher-level taxon?)</p></li>
</ul></li>
</ul>
<h2 id="improper-priors">Improper priors</h2>
<ul>
<li><p>There is no uniform distribution over the real numbers</p></li>
<li><p>But for Bayesian analysis, we can pretend that there is</p>
<ul>
<li>Using this sort of improper prior poses no problems (over and above the problems of specifying a uniform prior in the first place), as long as we can guarantee that the posterior distribution exists</li>
</ul></li>
</ul>
<h2 id="statistical-models">Statistical models</h2>
<ul>
<li><p>A statistical model allows us to calculate the <em>likelihood</em> of the data based on parameters</p>
<ul>
<li><p>Relationships between quantities, e.g.:</p></li>
<li><p>X is linearly related to Y</p></li>
<li><p>The variance of X is linearly related to Z</p></li>
<li><p>Distributions</p></li>
<li><p>X has a Poisson (or normal, or lognormal) distribution</p></li>
</ul></li>
</ul>
<h1 id="making-a-probability-model">Making a probability model</h1>
<h2 id="assumptions-1">Assumptions</h2>
<ul>
<li><p>We need enough assumptions to actually calculate the “likelihood” of our data given parameters</p></li>
<li><p>To make a probability model we need prior distributions for all of the parameters we wish to estimate</p></li>
<li><p>We then need to make explicit assumptions about how our data are generated, and calculate a likelihood for the data corresponding to any set of parameters</p></li>
</ul>
<h2 id="an-analytic-example">An analytic example</h2>
<ul>
<li><p>We count events over a period of time, and would like credible intervals (or a whole posterior distribution) for the underlying rate (assuming events are independent).</p></li>
<li><p>For each rate, our likelihood of observing <span class="math inline">\(N\)</span> events in time <span class="math inline">\(T\)</span> if the true rate is <span class="math inline">\(r\)</span> is a Poisson distribution with mean <span class="math inline">\(rT\)</span>:</p>
<ul>
<li><span class="math inline">\(\frac{(rT)^N \exp(-rT)}{N!}\)</span></li>
</ul></li>
<li><p>We choose an improper, uniform prior over <span class="math inline">\(\log r\)</span>, equivalent to <span class="math inline">\(\pi(r) = 1/r\)</span>.</p></li>
<li>The posterior distribution is then proportional to:
<ul>
<li><span class="math inline">\((rT)^{N-1} \exp(-rT)\)</span>, which gives a Gamma distribution with mean <span class="math inline">\(N/T\)</span> (the observed rate), and CV <span class="math inline">\(1/\sqrt{N}\)</span>.</li>
</ul></li>
</ul>
<p>This example is in the category of “easy problems”; the math is a bit hard (Calc II level), but no harder than the equivalent math for a frequentist approach, and the actual procedure is easy once you know how.</p>
<h1 id="mcmc-methods">MCMC methods</h1>
<ul>
<li><p>Bayesian methods are very flexible</p></li>
<li><p>We can write down reasonable priors, and likelihoods, to cover a wide variety of assumptions and situations</p></li>
<li><p>Unfortunately, we usually can’t <em>integrate</em> – calculate the denominator of Bayes’ formula</p></li>
<li><p>Instead we use <em>Markov chain Monte Carlo</em> methods to sample randomly from the posterior distribution</p>
<ul>
<li>Simple to do, but hard to know how long you have to simulate to get a good sample of the posterior distribution</li>
</ul></li>
</ul>
<h2 id="mcmc-sampling">MCMC sampling</h2>
<ul>
<li><p>Rules that assure that we will visit each point in parameter space in proportion to its likelihood … eventually</p></li>
<li><p>Checking convergence:</p>
<ul>
<li><p>Look at your parameter estimates: do they seem to have settled to bouncing back and forth) rather than going somewhere?</p></li>
<li><p>Repeat the whole process with a different starting point (in parameter space): do these “chains” converge?</p></li>
</ul></li>
</ul>
<h2 id="packages">Packages</h2>
<ul>
<li><p>There is a lot of software, including R packages, that will do MCMC sampling for you</p></li>
<li>We will give you examples
<ul>
<li><a href="Bayesian_regression_example.html">Bayesian regression example</a> using rjags</li>
</ul></li>
</ul>
<h1 id="sampling-from-the-posterior">Sampling from the posterior</h1>
<h2 id="great-power-great-responsibility">Great power ⇒ great responsibility</h2>
<ul>
<li>Once you have calculated (or estimated) a Bayesian posterior, you can calculate whatever you want!
<ul>
<li>In particular, you can attach a probability to any combination of the parameters</li>
<li>You can simulate a model forward in time and get credible intervals not only for the parameters, but what you expect to happen</li>
</ul></li>
</ul>
<h2 id="live-coding-example">Live coding example</h2>
<ul>
<li><a href="fev.R">R script</a></li>
<li><a href="fev.bug">model file</a></li>
</ul>
<div id="footer">
	<div style="text-align:center">
		<a href="index.html">Course home page</a>
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
		<a href="mailto:bio708qmee@gmail.com">Email the profs</a>
		&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
		<a href="logo.html">Logo information</a>
	</div>
</div>
</body>
</html>
