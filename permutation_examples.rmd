---
title: "Simple permutation tests in R"
date: "`r format(Sys.time(), '%H:%M %d %B %Y')`"
author: Ben Bolker
output: 
  html_document:
    toc: true
    toc_depth: 2
---

## Example: counting ant colonies

From Bio 708 class; data originally from Gotelli and Ellison *Primer of Ecology*:

```{r define_data}
forest <- c(9, 6, 4, 6, 7, 10)
field  <- c(12, 9, 12, 10)
ants <- data.frame(
  place=rep(c("field","forest"),
            c(length(field), length(forest))),
	colonies=c(field,forest)
)
```

### Visualization

Look at the data (with `stat_sum()` to visualize overlapping data points;
jittering is also a possibility, but `stat_sum()` is prettier).  `aes(size=..n..)` tells `stat_sum()` to use the number of overlapping points, not the proportion of points within a category, as the summary statistic; `scale_size_area()` tells ggplot to scale the area of the points proportional to the size (`breaks=1:2` tells it what values to show in the legend).  We don't really need the boxplot here, but shown for comparison (and to indicate that boxplots are just a bit silly for really small data sets; if you must show them, you should probably include the points as well, as shown here).

```{r antPlot,message=FALSE}
library("ggplot2"); theme_set(theme_bw())
ggplot(ants,aes(place,colonies))+
    stat_sum(aes(size=..n..),colour="darkgray")+
        scale_size_area(breaks=1:2,max_size=4)+
            geom_boxplot(fill=NA)
```

## Permutation tests

### Brute force

There are always trade-offs between simplicity, transparency, length of code, computational efficiency ...

The simplest way to do this would be something like:

```{r brute1}
set.seed(101) ## for reproducibility
res <- numeric(1000) ## set aside space for results
for (i in 1:1000) {
    colonyboot <- sample(c(field,forest)) ## scramble
    ## pick out forest & field samples
    forestboot <- colonyboot[1:length(forest)] 
    fieldboot <- colonyboot[(length(forest)+1):length(colonyboot)]
    ## compute & store difference in means
    res[i] <- mean(fieldboot)-mean(forestboot)
}
obs <- mean(field)-mean(forest)
hist(res,col="gray",las=1,main="")
abline(v=obs,col="red")
```

Since there aren't actually that many possible outcomes,
we could plot them this way:
```{r plot.ant.table}
par(las=1,bty="l")
plot(prop.table(table(round(res,2))),
     ylab="Proportion")
points(obs,0,pch=16,cex=1.5,col="red")
```

If we want to do a 2-tailed test, we have to decide whether
we are doubling the observed value 
or counting the area in both tails.


```{r ant_force}
2*mean(res>=obs)          ## doubling
mean(abs(res)>=abs(obs))  ## count both tails: matches lmPerm
```

## Using a t test

The standard parametric test to use here would be a $t$ test,
which is also equivalent to a 1-way ANOVA as executed by `lm()`
(except that for some reason they seem to use opposite signs
for the effect size):
```{r ant_t_test}
(tt <- t.test(colonies~place,data=ants,var.equal=TRUE))
summary(lm(colonies~place,data=ants))  ## matches
```

## Using `lmPerm`

```{r install_lmperm,eval=FALSE}
library("devtools")
install_version("lmPerm",version="1.1-2")
```
To install this package you will need compilation tools (for [Windows](http://cran.r-project.org/bin/windows/Rtools/) or [MacOS](http://r.research.att.com/tools/); if you are on Linux you probably have them already).

```{r ant_lmperm}
library("lmPerm")  ## load the package
summary(lmp(colonies~place,data=ants))
```

`lmp()` seems to automatically change the contrast settings
from the default treatment contrast to sum-to-zero contrasts,
so that the reported effect size is half what it was (3.75/2),
because it is computing the difference between the (unweighted)
average of the two groups and the first group (field).

Oddly, it seems to report the unadjusted $F$-statistic and
p-value for the full model ...

## Using `coin`

The `coin` package is big and complicated and powerful.  For each of the tests it provides, it allows a choice of whether to use differences of ranks or raw differences, and whether to use (1) *asymptotic* p-values (like the classic nonparametric tests: Kruskal-Wallis, Mann-Whitney, etc.); (2) *approximate* p-values (taking many random samples), or (3) *exact* p-values (effectively, generating all possible combinations).
```{r ant_coin}
library("coin")
## default: asymptotic
oneway_test(colonies~place,data=ants)
## exact distribution
oneway_test(colonies~place,data=ants,distribution="exact")
## approximate (random-sampling/Monte Carlo)
oneway_test(colonies~place,data=ants,distribution=approximate(B=9999))
```
    
## More general approach

(Should I break this out into a separate document?)

I'm going to do things in a *slightly* more complex way than JD did, to illustrate a more abstract but general approach.

Get `gtools` package and generate the combinations, as in the original example:
```{r get_comb}
library(gtools)
ind_comb <- combinations(nrow(ants), length(field))
```

Now write two functions. The first, `simfun()`, simulates a randomized data set given inputs (in this case, the input is a list of elements to be assigned to the "field" category).  We take the `colonies` column from the original `ants` data set and arrange the field-assigned colony counts first, and the non-field-assigned colony counts second.

```{r simfun}
simfun <- function(cc) {
  transform(ants,colonies=c(colonies[cc],colonies[-cc]))
}
```

The second function, `sumfun()`, takes a simulated data set and returns whatever summary statistic we want.  In this case I decided to use the $t$ statistic as computed by R.  (In many cases simple summary statistics can be computed more efficiently by doing it by hand, but it's often conceptually clearer to run *exactly the same test* that we would have used in the non-permutation analysis and extract the test statistic, which is usually stored as a list element called "statistic", from it.)

```{r sumfun}
sumfun <- function(dat) {
  t.test(colonies~place,data=dat,var.equal=TRUE)[["statistic"]]
}
```

```{r get_permdist}
ncomb <- nrow(ind_comb)
permdist <- numeric(ncomb)
for (i in 1:ncomb) {
    permdist[i] <- sumfun(simfun(ind_comb[i,]))
}
```
(this could also be done using R's `apply()` function).
What do we get, and how does it compare with the distribution we would expect from classical statistics, which is a $t$-distribution with `r tt$parameter` degrees of freedom?

```{r stats_hist}
obs_stat <- tt[["statistic"]]
hist(permdist,col="gray",breaks=30,freq=FALSE,main="")
curve(dt(x,df=tt[["parameter"]]),add=TRUE,col="red")
abline(v=obs_stat,col="blue")
```

One way to get the $p$-value:

```{r pval}
(obs_pval <- mean(abs(permdist)>=abs(obs_stat)))
```

* using `mean(permutations>=obs))` is a trick to calculate the proportion: the logical statement returns a logical (`FALSE`/`TRUE`) vector, which then gets converted to a 0/1 vector when you ask R to take the mean, so this is equivalent to counting the number of true values and dividing by the length ...
* I use `abs()` to get the two-tailed test (alternatively, I could double the one-tailed test $p$-value)
* **note** that because the permutation distribution is not completely symmetric, this is not quite the same as the result JD got, which is based on doubling the area of the upper tail: the lower tail is a little bit "skinnier" than the upper tail, so we get a smaller $p$-value by adding together the areas of the two tails instead of doubling the area of the upper tail: `2*mean(permdist>=obs_stat)` gives `r round(2*mean(permdist>=obs_stat),3)`.

This gives just the single $p$-value, which we can compare with the $p$-value we got from the classical test (`r round(tt$p.value,3)`)

Quantile/p-value plot:
         
```{r quantplot}
par(las=1,bty="l")
r <- sort(ceiling(rank(permdist)))/length(permdist)
pval <- 2*pt(sort(abs(permdist),decreasing=TRUE),lower.tail=FALSE,
                     df=tt$parameter)
plot(r,pval,xlab="Permutation p-value",ylab="Nominal p-value",
     type="s",log="xy")
abline(a=0,b=1,col="gray")
abline(h=tt$p.value,v=obs_pval,col=adjustcolor("red",alpha=0.5))
abline(h=0.05,col=adjustcolor("red",alpha=0.5),lty=2)
```

Wherever the black line is below/to the right of the gray 1:1 line, the permutation p-value is greater (more conservative than) the corresponding p-value from the classical $t$-test.  This is true for both the observed data (red lines) and for the $p=0.05$ cutoff (horizontal dashed line).

### Other approaches

### Brute-force resampling

If we aren't concerned about generating the exact set of combinations, we can just randomize the order of the response variable (i.e. permute the values): R's `sample()` function does this by default (it has many other uses, including sampling with replacement for bootstrap analyses).  

We just define a new version of `simfun()`.  Because we are picking a different value every time, we don't need to keep track of which sample we are on; `simfun()` doesn't need to take any arguments, and we can use R's `replicate()` function to generate as many permutation results as we want:

```{r simfun2}
simfun_rsamp <- function() {
  transform(ants,colonies=sample(colonies))
}
set.seed(101)
permdist_rsamp <- replicate(2000,sumfun(simfun_rsamp()))
```

The result isn't quite the same as the exact value derived above, but it's pretty close (close to the result we got before):

```{r simfun2_res}
mean(abs(permdist_rsamp)>=abs(obs_stat))
```

### Use difference between means as test statistic

```{r meandiff}
sumfun_diffmean <- function(dat) {
  with(dat,
    mean(colonies[place=="field"])-mean(colonies[place=="forest"]))
}
sumfun_diffmean(ants)  ## test
permdist_diffmean <- apply(ind_comb,
                    MARGIN=1,function(x) sumfun_diffmean(simfun(x)))
mean(abs(permdist_diffmean)>=abs(sumfun_diffmean(ants)))
```

This gives exactly the same result as the original approach, because there is a one-to-one relationship between differences between means and $t$ statistics ...

## Permutation tests of regression: reproductive skew data

Some data from Holly Kindsvater on reproductive skew in fish (????):

```{r skewdat}
skewdat <- read.csv("skewdat.csv")
library(ggplot2)
theme_set(theme_bw())
qplot(Size,skew,data=skewdat)+geom_smooth(method="lm")
```

```{r skewlm}
summary(lm(skew~Size,data=skewdat))
```
Can we trust this regression? Let's try a permutation test.

Since all the $x$ (`Size`) values
are unique, there are a total of `r nrow(skewdat)`! (factorial) possible permutations, or `r 1e32*round(factorial(nrow(skewdat))*1e-32)`, way too many to do by brute force (insert calculations here about what fraction of the job we will have done by the time the sun burns out ...)

```{r skew_funs}
simfun_rsamp2 <- function(respvar="skew",data=skewdat) {
  permdat <- data
  permdat[[respvar]] <- sample(permdat[[respvar]])
  permdat
}
sumfun_skew <- function(dat) {
  coef(lm(skew~Size,data=dat))["Size"]
}
```

```{r skew_permute,cache=TRUE}
set.seed(101)
permdist_skew <- replicate(8000,sumfun_skew(simfun_rsamp2()))
(skew_pval <- mean(abs(permdist_skew)>=abs(sumfun_skew(skewdat))))
```
The results are *very* close to the classical test result
(before trying this with 8000 replicates,
I tried a few times with 2000 replicates and found that the results varied between about 0.02 and 0.035 -- maybe JD was right ...)

We could also use `lmPerm` for this:

```{r lmPerm_regr}
summary(lmp(skew~Size,data=skewdat))
```

Or `coin`:

```{r coin_regr}
independence_test(skew~Size,data=skewdat,teststat="scalar",
                  distribution="asymptotic")
independence_test(skew~Size,data=skewdat,teststat="scalar",
                  distribution=approximate(B=9999))
```

Since the standard error of an estimated proportion is $\sqrt{p(1-p)/n}$, the *coefficient of variation* (ratio of the standard error to the mean estimate, $p$) is $\sqrt{(1-p)/(pn)}$.  Thus for an observed $p$-value, if we want to get the coefficient of variation down to a specified level $c$ (say 5%, so the confidence intervals are approximately $\pm$ 10% of the estimated $p$-value) then we need to take $n$ large enough so that $c = \sqrt{(1-p)/(pn)}$, or $n \approx (1-p)/(p c^2)$; if $p$ is small then this is further approximated by $1/(p c^2)$ (e.g. for a $p$-value of 0.05 accurate within $c=0.05$, we need $1/(0.5 \cdot 0.5^2) = 1/(0.5^3) = 20^3 = 8000$ samples (slightly fewer since we have neglected the $1-p$ term). If we wanted a similarly accurate answer for our current answer, with a $p$-value about half as large, we would need twice as many samples.
